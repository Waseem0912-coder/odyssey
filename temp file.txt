import os
import shutil
import torch

from transformers import AutoTokenizer, AutoModelForCausalLM
from llmcompressor.modifiers.quantization import GPTQModifier
from llmcompressor.transformers import oneshot

# ------------------------------------------------------------
# 1. Configuration
# ------------------------------------------------------------
MODEL_ID = "mistralai/Ministral-3-14B-Instruct-2512"
OUTPUT_DIR = "./Ministral-3-14B-Instruct-AWQ"

# Clean output directory manually (oneshot does NOT overwrite)
if os.path.exists(OUTPUT_DIR):
    print(f"Removing existing output dir: {OUTPUT_DIR}")
    shutil.rmtree(OUTPUT_DIR)

# ------------------------------------------------------------
# 2. Safety Check: Load Model (Text Path Only)
# ------------------------------------------------------------
print("Loading model for sanity check...")

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_ID,
    trust_remote_code=True,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)

# Simple forward pass to ensure vision modules don't break tracing
with torch.no_grad():
    inputs = tokenizer("Hello world", return_tensors="pt").to(model.device)
    _ = model(**inputs)

print("Model forward pass OK.")

# ------------------------------------------------------------
# 3. Define Quantization Recipe
# ------------------------------------------------------------
recipe = [
    GPTQModifier(
        scheme="W4A16",
        targets="Linear",
        # Ignore heads + all known vision components
        ignore=[
            "lm_head",
            "vision_encoder",
            "vision_tower",
            "vision_model",
            "multi_modal_projector",
            "modality_preprocessors",
        ],
        dampening_frac=0.01,
        observer="mse",
    )
]

# ------------------------------------------------------------
# 4. Run One-Shot Quantization
# ------------------------------------------------------------
print(f"Starting GPTQ quantization for {MODEL_ID}...")

oneshot(
    model=MODEL_ID,
    dataset="open_platypus",
    recipe=recipe,
    output_dir=OUTPUT_DIR,
    num_calibration_samples=512,
    max_seq_length=2048,
    trust_remote_code_model=True,  # CORRECT flag name
)

print(f"Quantization complete. Model saved to: {OUTPUT_DIR}")

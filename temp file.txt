#!/usr/bin/env python3
"""
Simple AWQ Quantization for Ministral-3-14B-Reasoning-2512

Minimal script - just run it to quantize the model.

Install first:
    uv pip install torch --index-url https://download.pytorch.org/whl/cu121
    uv pip install vllm llmcompressor "transformers>=5.0.0" accelerate datasets mistral-common
"""

import torch
from datasets import load_dataset
from transformers import Mistral3ForConditionalGeneration, AutoProcessor
from llmcompressor import oneshot
from llmcompressor.modifiers.awq import AWQModifier

# Configuration
MODEL_ID = "mistralai/Ministral-3-14B-Reasoning-2512"
OUTPUT_DIR = "./ministral-3-14b-awq"
NUM_SAMPLES = 256
MAX_SEQ_LEN = 2048

print(f"Loading model: {MODEL_ID}")
model = Mistral3ForConditionalGeneration.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
tokenizer = processor.tokenizer if hasattr(processor, 'tokenizer') else processor

print("Loading calibration data...")
ds = load_dataset("mit-han-lab/pile-val-backup", split=f"validation[:{NUM_SAMPLES}]")
ds = ds.shuffle(seed=42)

def tokenize(sample):
    return tokenizer(
        sample["text"][:2000],
        padding=False,
        max_length=MAX_SEQ_LEN,
        truncation=True,
        add_special_tokens=False,
    )

ds = ds.map(tokenize, remove_columns=ds.column_names)

print("Applying AWQ quantization...")
# AWQ recipe - ignore vision encoder and lm_head
recipe = AWQModifier(
    targets=["Linear"],
    scheme="W4A16",
    ignore=[
        "lm_head",
        "re:.*vision.*",
        "re:.*visual.*",
        "re:.*image.*",
        "re:.*multi_modal_projector.*",
    ],
)

oneshot(
    model=model,
    dataset=ds,
    recipe=recipe,
    max_seq_length=MAX_SEQ_LEN,
    num_calibration_samples=NUM_SAMPLES,
)

print(f"Saving to {OUTPUT_DIR}...")
model.save_pretrained(OUTPUT_DIR, save_compressed=True)
processor.save_pretrained(OUTPUT_DIR)

print("\n" + "="*50)
print("DONE! Testing generation...")
print("="*50)

inputs = tokenizer("Hello, what is 2+2?", return_tensors="pt").to(model.device)
with torch.no_grad():
    out = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(out[0], skip_special_tokens=True))

print(f"\nModel saved to: {OUTPUT_DIR}")
print(f"\nRun with vLLM:")
print(f"  vllm serve {OUTPUT_DIR} --tokenizer_mode mistral --config_format mistral --load_format mistral")
